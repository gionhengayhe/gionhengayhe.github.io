[
{
	"uri": "//localhost:1313/vi/",
	"title": "Data Lake on AWS: Clean Data with Lambda Triggers",
	"tags": [],
	"description": "",
	"content": "Data Lake on AWS: Clean Data with Lambda Triggers Tổng quan Workshop này hướng dẫn bạn tạo một hệ thống Data Lake trên AWS để thu thập, xử lý, làm sạch và truy vấn dữ liệu. Chúng ta sẽ bắt đầu bằng việc thu thập dữ liệu từ API, upload vào S3, sau đó kết hợp AWS Lambda trigger để tự động làm sạch dữ liệu ngay khi tải lên. Dữ liệu sẽ được đổ vào Glue Data Catalog để phân tích bằng Athena, chuyển đổi qua Glue Job, và trực quan hóa với QuickSight.\nMục đích Tìm hiểu về Data Lake: Giúp người tham gia hiểu rõ Data Lake là gì, cách xây dựng và vận hành Data Lake trên AWS.\nTự động hóa quy trình làm sạch dữ liệu: Đồng bộ các dữ liệu upload lên S3 và xử lý ngay lập tức bằng Lambda trigger.\nTăng cường kỹ năng phân tích dữ liệu: Tìm hiểu Glue Data Catalog, truy vấn Athena, và biểu diễn trực quan QuickSight.\nĐộc lập và linh hoạt: Xây dựng mô hình hợp nhất cho nhiều loại dữ liệu, đầu vào từ API, S3, hay dữ liệu ngoại.\nYêu cầu: Tài khoản AWS: User có quyền Admin và đã thiết lập billing.\nDữ liệu API: Link dữ liệu sẽ được cung cấp trong workshop.\nChi phí: AWS Free Tier: S3, Lambda, Athena AWS Glue Job: 0.44$/h for one session QuickSight: 30 days free "
},
{
	"uri": "//localhost:1313/vi/1-introduction/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "Tổng quan Data Lake là một thuật ngữ chuyên môn có liên quan đến Big Data (Dữ liệu lớn). Data Lake đơn giản là nơi chứa dữ liệu thô (chưa xử lý) chờ được xử lý phân tích và đưa ra các đánh giá nhận xét (insight).\nData Lake có các tính chất sau:\nThu thập mọi thứ – chứa tất cả dữ liệu dạng thô hoặc đã được xử lý trong khoảng thời gian dài. Đa người dùng – cho phép nhiều người dùng tinh chỉnh, khám phá và làm phong phú dữ liệu. Truy cập linh hoạt – hỗ trợ nhiều cách thức truy cập dữ liệu (access pattern) trên cơ sở hạ tầng dùng chung: lô (batch), tương tác, trực tuyến, - tìm kiếm, trong bộ nhớ và các công cụ xử lý khác. EventBridge là một dịch vụ serverless sử dụng các sự kiện để kết nối các thành phần ứng dụng với nhau, giúp bạn dễ dàng xây dựng các ứng dụng hướng sự kiện có thể mở rộng quy mô. Amazon Glue: là một dịch vụ ETL hoàn chỉnh. Bạn có thể sử dụng Glue Crawler để nhận diện dữ liệu của bạn và lưu trữ thông tin dữ liệu (metadata) liên quan (ví dụ: định nghĩa bảng và schema) trong Glue Data Catalog. Sau khi được phân loại, dữ liệu của bạn có thể được tìm kiếm ngay, truy vấn và sẵn sàng cho các công việc ETL.\nAWS Glue ETL có thể tạo mã để thực hiện công việc chuyển đổi dữ liệu và đưa dữ liệu vào vùng lưu trữ. AWS Glue có khả năng tạo mã Python có thể tùy chỉnh, tái sử dụng.\nSau khi các ETL Job của bạn đã sẵn sàng, chúng ta có thể tạo lịch để chạy trên môi trường Apache Spark có khả năng mở rộng lớn được quản lý bởi AWS Glue. Amazon Athena: một dịch vụ truy vấn tương tác được sử dụng để phân tích dữ liệu trong Amazon S3 với SQL tiêu chuẩn. Chúng ta chỉ cần trỏ đến dữ liệu của bạn trong Amazon S3, xác định schema và bắt đầu truy vấn bằng trình chỉnh sửa truy vấn tích hợp. Amazon Athena cho phép chúng ta khai thác tất cả dữ liệu của mình trong Amazon S3 mà không cần phải thiết lập các quy trình ETL phức tạp. Amazon Athena tính tiền dựa trên các truy vấn được chạy.\nAmazon Athena sử dụng Presto với hỗ trợ SQL ANSI và hoạt động với nhiều định dạng dữ liệu tiêu chuẩn, bao gồm CSV, JSON, ORC, Avro và Parquet. Athena được khuyến nghị cho nhu cầu truy vấn nhanh, nhưng nó cũng có thể xử lý các phân tích phức tạp, bao gồm các phép Join với lượng dữ liệu lớn, các window functions và mảng. Amazon QuickSight: một dịch vụ biểu diễn dữ liệu được quản lý hoàn toàn bởi AWS. Data source là một kho lưu trữ dữ liệu bên ngoài và bạn cần cấu hình việc truy cập dữ liệu trong kho dữ liệu bên ngoài này, ví dụ. Amazon S3, Amazon Athena, Salesforce, v.v.\nDataset xác định dữ liệu cụ thể trong Data source mà bạn muốn sử dụng. Ví dụ: Data source có thể là một bảng nếu bạn đang kết nối với Data source cơ sở dữ liệu. Nó có thể là một file nếu bạn đang kết nối với Data source Amazon S3.\nAnalysis là nơi chứa một tập hợp các Visual và câu chuyện có liên quan, ví dụ như tất cả các câu chuyện áp dụng cho một mục tiêu kinh doanh nhất định hoặc KPI.\nVisual là một biểu diễn đồ họa cho dữ liệu của bạn. Bạn có thể tạo nhiều loại Visual khác nhau trong một analysis, sử dụng các bộ dữ liệu và loại Visual khác nhau.\nDashboard là một trang bao gồm một hoặc nhiều Analysis chỉ cho phép xem mà bạn có thể chia sẻ với những người dùng Amazon QuickSight khác cho mục đích báo cáo. Dashboard lưu giữ cấu hình của bản Analysis tại thời điểm bạn xuất bản nó, bao gồm những thứ như lọc, tham số, điều khiển và thứ tự sắp xếp.\n"
},
{
	"uri": "//localhost:1313/vi/6-analysis-visualization/1-analysis-athena/",
	"title": "Phân tích với Athena",
	"tags": [],
	"description": "",
	"content": "Phân tích với Athena Vì Athena sử dụng AWS Glue Catalog để theo dõi nguồn dữ liệu, tất cả các bảng trong Glue có thể được truy vấn thông qua Athena.\nTruy cập AWS Management Console\nTìm Athena Chọn Athena Trong giao diện Athena:\nData Source, chọn AwsDataCatalog Database, chọn summitdb Chọn bảng processed-data Chọn Preview Table "
},
{
	"uri": "//localhost:1313/vi/4-create-datalog/1-create-glue-crawler/",
	"title": "Tạo AWS Glue Crawler",
	"tags": [],
	"description": "",
	"content": "Tạo Glue Crawler Truy cập AWS Management Console Tìm AWS Glue. Chọn AWS Glue. Trong giao diện AWS Glue, chọn Crawlers. Chọn Create Crawler. Trong giao diện Add Crawler, nhập Crawler name là summitcrawler và chọn Next. Đối với Add data source, chọn S3. Chọn đường dẫn S3 thông qua Browse. Chọn: datalate-bucket-demo/cleaned-data. Đồng thời, chọn Crawl new sub-folders only và Add an S3 data source. Sau khi thêm nguồn dữ liệu, chọn Next. Đối với IAM role, chọn AWSGlueServiceDefault. Sau đó, chọn Next. Đối với Target database, thực hiện Add database. Tạo cơ sở dữ liệu bằng cách nhập tên cơ sở dữ liệu là summitdb và chọn Create database. Sau khi tạo cơ sở dữ liệu, chọn cơ sở dữ liệu và chọn Next. Xem lại cấu hình và chọn Create crawler. Tạo crawler thành công. Sau đó, chọn Run crawler. Mất khoảng 2-3 phút để chạy crawler. Khi bạn thấy trạng thái crawler là Ready. Trong giao diện AWS Glue, chọn Table, và bạn sẽ thấy 2 bảng dữ liệu. Chọn bảng dữ liệu post. Chọn bảng dữ liệu comment. Khám phá chi tiết của bảng dữ liệu. "
},
{
	"uri": "//localhost:1313/vi/2-preparation-steps/1-glue-role/",
	"title": "Tạo Glue Service Role",
	"tags": [],
	"description": "",
	"content": "Tạo Glue Service Role Trong bước này, chúng ta sẽ điều hướng đến IAM Console và tạo một role cho dịch vụ Glue. Role này sẽ cho phép AWS Glue truy cập dữ liệu trong S3 và tạo các đối tượng cần thiết trong Glue Catalog.\nTruy cập AWS Management Console\nTìm kiếm IAM Chọn IAM Trong console IAM:\nChọn Roles Chọn Create role Trong bước Select trusted entity:\nChọn AWS Service Đối với Use case, chọn Glue Nhấp vào Next Trong bước Add permissions:\nTìm kiếm chính sách AmazonS3FullAccess Chọn chính sách AmazonS3FullAccess Nhấp vào Next Tương tự như bước 4:\nTìm kiếm chính sách AWSGlueServiceRole Chọn chính sách AWSGlueServiceRole Trong giao diện Role details:\nĐối với Role name, nhập AWSGlueServiceRoleDefault Trong bước Add permissions:\nXem lại hai chính sách Nhấp vào Create role Chúng ta đã hoàn thành việc tạo IAM role\n"
},
{
	"uri": "//localhost:1313/vi/3-lambda-setup/1-creating-bucket/",
	"title": "Tạo một S3 Bucket",
	"tags": [],
	"description": "",
	"content": "Tải dữ liệu Nhấp vào đây: Post data Lưu với Ctrl + S Nhấp vào đây: Comment data Lưu với Ctrl + S Trên tệp .json vừa tải xuống, bạn có thể thay đổi mã để kiểm tra trường hợp sử dụng Làm sạch dữ liệu Bạn có thể sửa đổi dữ liệu cho mục đích kiểm tra: xóa một số bản ghi, đặt một số trường thành null, hoặc thay đổi giá trị id hoặc postId từ số thành chuỗi.\nTạo S3 Bucket Truy cập AWS Management Console\nTìm kiếm S3 Chọn S3 Trong giao diện S3\nChọn buckets Chọn Create bucket Trong giao diện Create bucket\nĐối với tên Bucket, nhập datalake-demo-bucket Nhấp vào Create bucket Đã tạo bucket thành công\nChọn bucket mới tạo Trong giao diện bucket mới\nChọn Create folder Trong giao diện Create folder\nĐối với tên thư mục, nhập raw-data Đã tạo thư mục thành công Tại thư mục raw-data\nTạo các thư mục post và comment Sau khi tạo thành công 2 thư mục, chúng ta sẽ không tải lên tệp ngay lập tức mà sẽ tiến hành bước tiếp theo, đó là Tạo Lambda Function để xử lý việc làm sạch dữ liệu\n"
},
{
	"uri": "//localhost:1313/vi/2-preparation-steps/",
	"title": "Các bước chuẩn bị",
	"tags": [],
	"description": "",
	"content": "Các bước chuẩn bị Chúng ta sẽ chuẩn bị để tạo một IAM role cho AWS Glue.\nNội dung Tạo Glue service role Tạo Lambda function role Tạo Policy "
},
{
	"uri": "//localhost:1313/vi/4-create-datalog/2-data-check/",
	"title": "Kiểm tra dữ liệu",
	"tags": [],
	"description": "",
	"content": "Kiểm tra dữ liệu Truy cập AWS Management Console\nTìm S3 Chọn S3 Trong giao diện S3\nChọn Buckets Chọn datalake-bucket-demo Chúng ta sẽ tạo một thư mục cho Athena\nChọn Create folder Trong giao diện Create folder\nFolder name, nhập Athena Chọn Create folder Tạo thư mục thành công\nTruy cập AWS Management Console\nTìm Athena Chọn Athena Trong giao diện Athena\nChọn View settings để thiết lập đường dẫn lưu kết quả truy vấn Trong giao diện Amazon Athena\nChọn Settings Chọn Manage Chọn đường dẫn đến thư mục Athena vừa tạo, sau đó chọn Choose\nQuay lại giao diện Manage settings, kiểm tra lại và chọn Save Chúng ta sử dụng Amazon Athena để truy vấn dữ liệu\nData Source, chọn AwsDataCatalog Database, chọn summitdb Chọn bảng comment Chọn Preview Table Thực hiện truy vấn 10 dòng dữ liệu từ bảng comment trong cơ sở dữ liệu summitdb\nTruy vấn thành công Kiểm tra dữ liệu Kiểm tra dữ liệu bảng comment nơi postid = 10 Kiểm tra dữ liệu bảng post limit 10; Bảng comment join bảng post "
},
{
	"uri": "//localhost:1313/vi/3-lambda-setup/2-creating-lambda/",
	"title": "Tạo Lambda function",
	"tags": [],
	"description": "",
	"content": "Tạo Lambda function Tạo Lambda function cho dữ liệu bài đăng Truy cập AWS Management Console\nTìm kiếm Lambda Chọn Lambda Chọn Create a function Tại function name: Clean-PostData\nTại runtime, chọn Python 3.13 Thay đổi execution role mặc định, chọn Use an existing role\nChọn AWSLambdaRoleDefault\nNhấp vào Create function Thêm mã này\nimport json\rimport boto3\rdef lambda_handler(event, context):\rtry:\rbucket_name = event[\u0026#39;detail\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;]\rfile_key = event[\u0026#39;detail\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;]\rresponse = s3.get_object(Bucket=bucket_name, Key=file_key)\rraw_data = response[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;)\rposts = json.loads(raw_data)\rcleaned_data = []\rfor post in posts:\rif (isinstance(post.get(\u0026#39;userId\u0026#39;), int) and\risinstance(post.get(\u0026#39;id\u0026#39;), int) and\risinstance(post.get(\u0026#39;title\u0026#39;), str) and\risinstance(post.get(\u0026#39;body\u0026#39;), str) and\rcleaned_data.append(post))\rcleaned_file_key = file_key.replace(\u0026#39;raw-data/post\u0026#39;, \u0026#39;cleaned-data/post\u0026#39;)\rcleaned_file_content = \u0026#34;\\n\u0026#34;.join(json.dumps(post) for post in cleaned_data)\rs3.put_object(Bucket=bucket_name, Key=cleaned_file_key, Body=cleaned_file_content)\rprint(f\u0026#34;Cleaned data uploaded to :s3://{bucket_name}/{cleaned_file_key}\u0026#34;)\rexcept Exception as e:\rprint(f\u0026#34;Error processing file {file_key}: {e}\u0026#34;)\rraise e Vì AWS Glue không thể diễn giải JSON Array như JSON Objects để ánh xạ lược đồ, chúng ta cần chuyển đổi dữ liệu thành định dạng JSON Object.\nMã này sẽ kiểm tra dữ liệu đến, và nếu dòng dữ liệu không ở định dạng chính xác, nó sẽ bỏ qua dữ liệu đó.\nuserId phải là số nguyên.\nid phải là số nguyên.\ntitle phải là một chuỗi.\nbody phải là một chuỗi.\nNhấp vào Deploy(Ctrl+Shift+U) Tạo Lambda function cho dữ liệu bình luận Tương tự như Tạo Lambda function cho dữ liệu bài đăng, tạo một Lambda Function Clean-CommentData\nSử dụng mã này:\nimport json\rimport boto3\rdef lambda_handler(event, context):\rtry:\rbucket_name = event[\u0026#39;detail\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;]\rfile_key = event[\u0026#39;detail\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;]\rresponse = s3.get_object(Bucket=bucket_name, Key=file_key)\rraw_data = response[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;)\rcomments = json.loads(raw_data)\rcleaned_data = []\rfor comment in comments:\rif (isinstance(post.get(\u0026#39;postId\u0026#39;), int) and\risinstance(post.get(\u0026#39;id\u0026#39;), int) and\risinstance(post.get(\u0026#39;name\u0026#39;), str) and\risinstance(post.get(\u0026#39;email\u0026#39;), str) and\risinstance(post.get(\u0026#39;body\u0026#39;), str) and\rcleaned_data.append(post))\rcleaned_file_key = file_key.replace(\u0026#39;raw-data/comment\u0026#39;, \u0026#39;cleaned-data/comment\u0026#39;)\rcleaned_file_content = \u0026#34;\\n\u0026#34;.join(json.dumps(comment) for post in cleaned_data)\rs3.put_object(Bucket=bucket_name, Key=cleaned_file_key, Body=cleaned_file_content)\rprint(f\u0026#34;Cleaned data uploaded to :s3://{bucket_name}/{cleaned_file_key}\u0026#34;)\rexcept Exception as e:\rprint(f\u0026#34;Error processing file {file_key}: {e}\u0026#34;)\rraise e "
},
{
	"uri": "//localhost:1313/vi/2-preparation-steps/2-lambda-role/",
	"title": "Tạo Lambda role",
	"tags": [],
	"description": "",
	"content": "Tạo Glue Service Role Trong bước này, chúng ta sẽ điều hướng đến IAM Console và tạo một role cho dịch vụ Lambda function. Role này sẽ cho phép Lambda function truy cập dữ liệu trong S3, làm sạch dữ liệu và tạo các đối tượng cần thiết trong Lambda Catalog.\nTruy cập AWS Management Console\nTìm kiếm IAM Chọn IAM Trong console IAM:\nChọn Roles Chọn Create role Trong bước Select trusted entity:\nChọn AWS Service Đối với Use case, chọn Lambda Nhấp vào Next Trong bước Add permissions:\nTìm kiếm chính sách AmazonS3FullAccess Chọn chính sách AmazonS3FullAccess Nhấp vào Next Tương tự như bước 4:\nTìm kiếm chính sách AWSLambda_FullAccess Chọn chính sách AWSLambda_FullAccess Tìm kiếm chính sách AWSLambdaBasicExecutionRole Chọn chính sách AWSLambdaBasicExecutionRole Trong giao diện Role details:\nĐối với Role name, nhập AWSLambdaRoleDefault Trong bước Add permissions:\nXem lại ba chính sách Nhấp vào Create role Chúng ta đã hoàn thành việc tạo IAM role\n"
},
{
	"uri": "//localhost:1313/vi/6-analysis-visualization/2-visualize-quicksight/",
	"title": "Trực quan hóa với QuickSight",
	"tags": [],
	"description": "",
	"content": "Trực quan hóa với QuickSight Trong bước này, chúng ta sẽ thực hiện trực quan hóa bằng QuickSight.\nĐăng nhập vào Amazon QuickSight Console và hoàn thành quá trình đăng ký. Bạn có thể tham khảo quy trình đăng ký tại đây.\nTruy cập AWS Management Console\nTìm QuickSight Chọn QuickSight Lưu ý: Chú ý đến Region bạn đang sử dụng. Kiểm tra và chuyển đổi Region nếu cần thiết để tránh lỗi.\nTrong giao diện QuickSight, chọn Manage QuickSight Cấu hình Permissions\nChọn Security \u0026amp; permissions Chọn Manage Chọn dịch vụ và chọn Amazon S3\nChọn S3 Buckets Linked To QuickSight Account Chọn datalake-bucket-demo Chọn Finish Chọn và xem lại Role và dịch vụ. Sau đó, chọn Save Trong giao diện QuickSight:\nChọn Datasets Chọn New dataset Trong giao diện Create dataset:\nChọn nguồn từ Athena Cấu hình nguồn dữ liệu:\nData source name, nhập summitdemo Chọn Validate Connection. Nếu kết nối thành công, nó sẽ hiển thị SSL is enabled Chọn Create data source Chọn Bảng:\nCatalog, chọn AWSDataCatalog Database, nhập summitdb Table, chọn processed_data Chọn Select Trong bước Finish dataset creation:\nChọn Directly query your data Chọn Visualize Chọn Create Sử dụng Amazon QuickSight để trực quan hóa dữ liệu đã chuyển đổi:\nĐếm bình luận cho mỗi bài đăng\nNhấp vào Pie Chart Group/color: postid Value: commentid(Count) Đếm số bài đăng của mỗi người dùng\nGroup/color: postuserid Value: postid(Count) Nếu bạn gặp lỗi như hoặc Đi đến IAM Role Console Tìm aws-quicksight-service-role-\u0026hellip; Nhấp vào Add Permissions Nhấp vào Attach policies Tìm kiếm Quicksight và chọn các chính sách này: "
},
{
	"uri": "//localhost:1313/vi/2-preparation-steps/3-policy/",
	"title": "Tạo Policy",
	"tags": [],
	"description": "",
	"content": "Tạo Policy Truy cập AWS Management Console\nTrước tiên, bạn cần truy cập AWS Management Console tại https://aws.amazon.com/console/. Đăng nhập vào tài khoản AWS\nĐăng nhập vào tài khoản AWS của bạn bằng tên đăng nhập và mật khẩu. Mở trang IAM (Identity and Access Management)\nSau khi đăng nhập thành công, chọn dịch vụ “IAM” bằng cách tìm kiếm nó trong thanh tìm kiếm hoặc tìm trong danh sách các dịch vụ. Tạo một Policy\nTrong giao diện IAM, chọn “Policies” ở menu bên trái. Bấm vào nút “Create policy” để tạo một policy mới. Paste JSON Policy\nTrong trang “Create policy”, chọn phần “JSON” và dán nội dung JSON policy bạn đã cung cấp ở trên vào ô văn bản. Đảm bảo rằng JSON policy đã được đánh dấu là hợp lệ và không có lỗi cú pháp. {\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: \u0026#34;iam:PassRole\u0026#34;,\r\u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:iam::AccountID:role/AWSGlueServiceRoleDefault\u0026#34;\r}\r]\r} Đặt tên cho Policy Tiếp theo, bạn cần đặt tên cho policy. Điền tên vào trường “Name”. Cung cấp mô tả (tùy chọn) cho policy nếu bạn muốn. Kiểm tra và tạo Policy Bấm vào nút “Review policy” để kiểm tra lại thông tin policy của bạn. Sau khi kiểm tra kỹ, nếu không có lỗi, hãy bấm vào nút “Create policy” để tạo policy mới. Đính kèm Policy vào AWSGlueServiceRoleDefault Sau khi policy được tạo thành công, bạn cần đính kèm nó vào role AWSGlueServiceRoleDefault. Chọn “Roles” ở menu bên trái trong giao diện IAM. Tìm và chọn role “AWSGlueServiceRoleDefault”. Trong trang chi tiết của role, chọn tab “Permissions” và sau đó bấm vào nút “Add inline policy”. Trong trang “Create policy”, tìm kiếm và chọn policy bạn vừa tạo. Bấm vào nút “Next: Review policy” để kiểm tra lại. Sau khi kiểm tra và không có lỗi, hãy bấm vào nút “Create policy” để đính kèm policy vào role. "
},
{
	"uri": "//localhost:1313/vi/3-lambda-setup/3-creating-eventbridge/",
	"title": "Thiết lập EventBridge và S3 Trigger",
	"tags": [],
	"description": "",
	"content": "Tạo EventBridge Truy cập AWS Management Console\nTìm kiếm EventBridge Chọn Amazon EventBridge Tại Amazon EventBridge Console, chọn Create Rule Tại name: TriggerCommentData Nhấp vào Next Tại creation method, chọn Custom pattern Tại event pattern: {\r\u0026#34;source\u0026#34;: [\u0026#34;aws.s3\u0026#34;],\r\u0026#34;detail-type\u0026#34;: [\u0026#34;Object Created\u0026#34;],\r\u0026#34;detail\u0026#34;:{\r\u0026#34;bucket\u0026#34;:{\r\u0026#34;name\u0026#34;: [\u0026#34;datalake-bucket-demo\u0026#34;]\r},\r\u0026#34;object\u0026#34;:{\r\u0026#34;key\u0026#34;:[{\r\u0026#34;prefix\u0026#34;: \u0026#34;raw-data/comment\u0026#34;\r}]\r}\r}\r} Nhấp vào Next Tại Select a target: Lambda function Function: Clean-CommentData Nhấp vào Next Xem lại rule, nhấp vào Create rule Tương tự như bước 2-6, tạo rule TriggerPostData\nSử dụng pattern này:\n{\r\u0026#34;source\u0026#34;: [\u0026#34;aws.s3\u0026#34;],\r\u0026#34;detail-type\u0026#34;: [\u0026#34;Object Created\u0026#34;],\r\u0026#34;detail\u0026#34;:{\r\u0026#34;bucket\u0026#34;:{\r\u0026#34;name\u0026#34;: [\u0026#34;datalake-bucket-demo\u0026#34;]\r},\r\u0026#34;object\u0026#34;:{\r\u0026#34;key\u0026#34;:[{\r\u0026#34;prefix\u0026#34;: \u0026#34;raw-data/post\u0026#34;\r}]\r}\r}\r} Sau khi tạo rule TriggerPostData, chúng ta sẽ có: Quay lại Lambda Function Clean-PostData và Clean-CommentData, bạn sẽ thấy: Thiết lập gửi thông báo đến EventBridge cho tất cả sự kiện trong S3 Mở console S3 datalake-bucket-demo, nhấp vào Properties Tại Amazon EventBridge, nhấp vào Edit Bật Send notifications to EventBridge for all events in this bucket, sau đó nhấp vào Save changes Chúc mừng, bạn đã tạo thành công một trigger để kích hoạt Lambda khi lắng nghe sự kiện tải lên của S3 với EventBridge. Bây giờ chúng ta sẽ chuyển sang bước tiếp theo: Tải lên dữ liệu và xem kết quả. "
},
{
	"uri": "//localhost:1313/vi/3-lambda-setup/",
	"title": "Thiết lập Pipeline Xử lý Dữ liệu",
	"tags": [],
	"description": "",
	"content": "Thiết lập Pipeline Xử lý Dữ liệu Các bước: 3.1. Tạo một S3 Bucket 3.2. Tạo Lambda Function 3.3. Thiết lập EventBridge và S3 Trigger 3.4. Tải lên dữ liệu và xem kết quả "
},
{
	"uri": "//localhost:1313/vi/3-lambda-setup/4-watch-result/",
	"title": "Tải lên dữ liệu và xem kết quả",
	"tags": [],
	"description": "",
	"content": "Tải lên dữ liệu Mở thư mục: datalake-bucket-demo/raw-data/post Nhấp vào Upload Nhấp vào Add files Chọn file post.json bạn đã tải xuống tại 3.1. Tạo S3 Bucket Nhấp vào Upload Mở Lambda Function Clean-PostData, tại Monitor, nhấp vào View CloudWatch logs Chọn Log stream mới nhất dựa trên dấu thời gian. Bên trong các sự kiện log của Log Stream đã chọn, tìm thông báo:\nCleaned data uploaded to s3://datalake-bucket-demo/cleaned-data/post/post.json. Điều đó có nghĩa là Lambda function của bạn đã thực thi thành công và tải lên dữ liệu đã được làm sạch. Quay lại S3 Console và điều hướng đến bucket datalake-bucket-demo. Tìm thư mục cleaned-data/post/. Bên trong thư mục này, bạn sẽ tìm thấy một file có tên post.json. Tải xuống file post.json để xem xét nội dung của nó.\nBạn sẽ nhận thấy rằng dữ liệu đã được làm sạch, với bất kỳ mục không hợp lệ hoặc sai định dạng nào đã bị xóa, và được cấu trúc chính xác như các đối tượng JSON.\nĐiều này đảm bảo dữ liệu đã sẵn sàng để AWS Glue suy ra lược đồ và xử lý nó một cách chính xác.\nraw-data/post.json của tôi: Trong file post.json raw-data của tôi, bạn sẽ thấy:\r1. Object 1 thiếu trường body.\r2. Object 2 thiếu trường title.\r3. Trường id trong object 3 là một chuỗi thay vì một số.\r4. Trường userId trong object 4 là một chuỗi thay vì một số.\r5. File raw-data là một mảng các đối tượng JSON được cấu trúc như:\r[{Object 1}, {Object 2}, ..., {Object n}].\rcleaned-data/post.json của tôi: Trong file cleaned-data/post.json, bạn sẽ thấy:\r1. Object 1, 2, 3, 4 đã bị xóa.\r2. Dữ liệu bây giờ được định dạng như {Object 1}, {Object 2}, ..., {Object n} mà không được bọc trong một mảng.\rTương tự như tải lên post.json, tải lên comment.json trong raw-data/comment/.\nĐó là cách bạn làm sạch dữ liệu bằng AWS Lambda và EventBridge. Bây giờ, chúng ta hãy chuyển sang bước tiếp theo: Tạo Data Catalog.\n"
},
{
	"uri": "//localhost:1313/vi/4-create-datalog/",
	"title": "Tạo Data Catalog",
	"tags": [],
	"description": "",
	"content": "Tạo Data Catalog Nội dung: 4.1. Tạo Glue Crawler\n4.2. Kiểm tra dữ liệu\n"
},
{
	"uri": "//localhost:1313/vi/5-data-transformation/",
	"title": "Chuyển đổi dữ liệu",
	"tags": [],
	"description": "",
	"content": "\rTrong một khoảnh khắc ngớ ngẩn, em chỉ ghi hình được một góc nhỏ của màn hình, nên một phần lớn của đoạn video đã biến mất. Tệ hơn nữa, em lại vô tình xóa sạch toàn bộ file dự án, nên không thể làm lại nhanh chóng được. Một vùng ghi hình tí hon và một nút xoá to đùng? Đúng là combo \u0026ldquo;hoàn hảo\u0026rdquo;! Chắc đây là cái người ta gọi là “trải nghiệm học hỏi,” nhưng mà đúng là một trải nghiệm khá đắt giá đấy.\nTạo SageMaker Notebook Truy cập AWS Management Console Tìm AWS Glue Chọn AWS Glue Chọn Notebooks Chọn Notebook Nhập tên notebook là notebook\nChọn IAM role Chọn Create notebook Đợi khoảng 2-3 phút, và notebook sẽ được tạo. Chuyển đổi dữ liệu Trong Notebook\nImport thư viện: SparkContext GlueContext boto3 awsglue import sys\rfrom awsglue.transforms import *\rfrom awsglue.utils import getResolvedOptions\rfrom pyspark.context import SparkContext\rfrom awsglue.context import GlueContext\rfrom awsglue.job import Job\rimport boto3\rimport time Tiếp theo chúng ta bắt đầu khám phá dữ liệu\nKhởi tạo Spark và Glue Contexts sc = SparkContext.getOrCreate()\rglueContext = GlueContext(sc)\rspark = glueContext.spark_session\rjob = Job(glueContext) Tải bảng Comment và xem Schema\ncomment_data = glueContext.create_dynamic_frame.from_catalog(database=\u0026#39;summitdb\u0026#39;, table_name=\u0026#39;comment\u0026#39;)\rcomment_data.printSchema() Tải bảng Post và xem schema\npost_data = glueContext.create_dynamic_frame.from_catalog(database=\u0026#39;summitdb\u0026#39;, table_name=\u0026#39;post\u0026#39;)\rpost_data.printSchema() Đếm số dòng trong bảng post và comment\nprint(\u0026#39;post_data (Count) = \u0026#39; + str(post_data.count()))\rprint(\u0026#39;comment_data (Count) = \u0026#39; + str(comment_data.count())) Xem trước post_data\npost_data.toDF().show(5) Xem trước comment_data\ncomment_data.toDF().show(5) Hiển thị Comments nơi postId = 10\ncomment_data.toDF().createOrReplaceTempView(\u0026#39;comment\u0026#39;)\rfilter_postidDF = spark.sql(\u0026#34;select \\* from comment where postid = 10\u0026#34;)\rprint(\u0026#34;PostId = 10 (count): \u0026#34; + str(filter_postidDF.count()))\rfilter_postidDF.show(5) Hiển thị Posts nơi userid = 10\npost_data.toDF().createOrReplaceTempView(\u0026#39;post\u0026#39;)\rfilter_useridDF = spark.sql(\u0026#34;select \\* from post where userid = 10\u0026#34;)\rprint(\u0026#34;UserId = 10 (count): \u0026#34; + str(filter_useridDF.count()))\rfilter_useridDF.show(5) Join comment_data(postid) với post_data(id) và xem xét joined_data\njoined_data = Join.apply(comment_data, post_data, \u0026#39;postId\u0026#39;, \u0026#39;id\u0026#39;) joined_data.printSchema() joined_data.toDF().show(5) Đổi tên cột để rõ ràng hơn\nrenamed_df = (\rjoined_data.toDF()\r.withColumnRenamed(\u0026#34;.id\u0026#34;, \u0026#34;commentId\u0026#34;)\r.withColumnRenamed(\u0026#34;.body\u0026#34;, \u0026#34;commentBody\u0026#34;)\r.withColumnRenamed(\u0026#34;id\u0026#34;, \u0026#34;postId\u0026#34;)\r.withColumnRenamed(\u0026#34;title\u0026#34;, \u0026#34;postTitle\u0026#34;)\r.withColumnRenamed(\u0026#34;name\u0026#34;, \u0026#34;commenterName\u0026#34;)\r.withColumnRenamed(\u0026#34;email\u0026#34;, \u0026#34;commenterEmail\u0026#34;)\r.withColumnRenamed(\u0026#34;body\u0026#34;, \u0026#34;postBody\u0026#34;)\r.withColumnRenamed(\u0026#34;userId\u0026#34;, \u0026#34;postUserId\u0026#34;))\rrenamed_df.show(5) Chuyển đổi DataFrame trở lại DynamicFrame\nfrom awsglue.dynamicframe import DynamicFrame\rnew_joined_data = DynamicFrame.fromDF(\rrenamed_df,\rglueContext,\r\u0026#34;new_joined_data\u0026#34;\r) Ghi dữ liệu đã chuyển đổi vào S3\ntry:\rdatasink = glueContext.write dynamic from options(\rframe=new_joined_data,\rconnection_type=\u0026#34;s3\u0026#34;,\rconnection_options={\r\u0026#34;path\u0026#34;: \u0026#34;s3://datalake-bucket-demo/cleaned-data/processed-data/\u0026#34;},\rformat=\u0026#34;parquet\u0026#34;)\rprint(\u0026#34;Transform data written to S3\u0026#34;)\rexcept Exception as e:\rprint(\u0026#34;Error writing to S3: \u0026#34; + str(e)) Dữ liệu đã chuyển đổi đã được ghi vào datalake-bucket-demo/cleaned-data/processed-data/ Kích hoạt và giám sát Glue Crawler\nglueclient = boto3.client(\u0026#39;glue\u0026#39;, region_name=\u0026#39;us-ease-1\u0026#39;)\rresponse = glueclient.start_crawler(Name=\u0026#34;summitcrawler\u0026#34;)\rprint(\u0026#39;---\u0026#39;)\rcrawler_state = \u0026#39;\u0026#39;\rwhile crawler_state 1= \u0026#39;STOPPING\u0026#39;:\rresponse = glueclient.get_crawler(Name=\u0026#34;summitcrawler\u0026#34;)\rcrawler_state = str(response[\u0026#39;Crawler\u0026#39;][\u0026#39;State\u0026#39;])\rtime.sleep(1)\rprint(\u0026#39;Crawler stopped\u0026#39;)\rprint (\u0026#39;---\u0026#39;) Kiểm tra Log events của Crawler\nMở summitcrawler trong Console AWS Glue Crawler Đợi 2 phút để summitcrawler chạy Chọn lần chạy Crawlers mới nhất và nhấp vào ViewCloudWatch logs Vì một số lỗi tôi không biết, bảng Processed-data không được tạo khi Summitcrawler chạy, vì vậy chúng ta cần tạo một AWS Glue Crawler mới cho Processed-data\rTruy cập AWS Management Console\nTìm AWS Glue. Chọn AWS Glue. Trong giao diện AWS Glue, chọn Crawlers. Chọn Create Crawler. Trong giao diện Add Crawler, nhập Crawler name là joined-crawler và chọn Next. Tại Add data source, chọn S3. Chọn S3 path thông qua Browse. Chọn: datalate-bucket-demo/cleaned-data/processed-data. Đồng thời, chọn Crawl new sub-folders only và Add an S3 data source. Sau khi thêm data source, chọn Next.\nCác bước khác như tạo summitcrawler Chạy joined-crawler\nKiểm tra bảng processed-data\nBạn sẽ thấy processed-data có schema là bảng post được join với bảng comment\n"
},
{
	"uri": "//localhost:1313/vi/6-analysis-visualization/",
	"title": "Phân tích và trực quan hóa",
	"tags": [],
	"description": "",
	"content": "Phân tích và trực quan hóa Tổng quan về Amazon Athena Amazon Athena là một dịch vụ truy vấn tương tác được sử dụng để phân tích dữ liệu trong Amazon S3 bằng SQL tiêu chuẩn. Chúng ta chỉ cần trỏ đến dữ liệu của bạn trong Amazon S3, xác định lược đồ và bắt đầu truy vấn với trình soạn thảo truy vấn tích hợp. Amazon Athena cho phép chúng ta khai thác tất cả dữ liệu của mình trong Amazon S3 mà không cần phải thiết lập các quy trình ETL phức tạp. Amazon Athena tính phí dựa trên các truy vấn được chạy.\nAmazon Athena sử dụng Presto với hỗ trợ SQL ANSI và hoạt động với nhiều định dạng dữ liệu tiêu chuẩn, bao gồm CSV, JSON, ORC, Avro và Parquet. Athena được khuyến nghị cho nhu cầu truy vấn nhanh, nhưng nó cũng có thể xử lý các phân tích phức tạp, bao gồm các phép join lớn, các window function và mảng. Tổng quan về Amazon QuickSight Amazon Quick Sight là một dịch vụ biểu diễn dữ liệu được quản lý hoàn toàn bởi AWS. Nguồn dữ liệu (Data source) là kho lưu trữ dữ liệu bên ngoài, và bạn cần cấu hình cách truy cập dữ liệu trong kho lưu trữ bên ngoài này, ví dụ như Amazon S3, Amazon Athena, Salesforce, v.v.\nTập dữ liệu (Dataset) xác định dữ liệu cụ thể trong Nguồn dữ liệu mà bạn muốn sử dụng. Ví dụ, Nguồn dữ liệu có thể là một bảng nếu bạn đang kết nối với cơ sở dữ liệu. Nó có thể là một tệp nếu bạn đang kết nối với một Nguồn dữ liệu Amazon S3.\nPhân tích (Analysis) chứa một tập hợp các Hình ảnh trực quan và câu chuyện có liên quan, ví dụ như cho một mục tiêu kinh doanh cụ thể hoặc KPI.\nHình ảnh trực quan (Visual) là biểu diễn đồ họa của dữ liệu của bạn. Bạn có thể tạo các loại Hình ảnh trực quan khác nhau trong một phân tích, sử dụng các tập dữ liệu và loại Hình ảnh trực quan khác nhau.\nBảng điều khiển (Dashboard) là một trang bao gồm một hoặc nhiều Phân tích chỉ để xem, mà bạn có thể chia sẻ với người dùng Amazon QuickSight khác cho mục đích báo cáo. Bảng điều khiển lưu giữ cấu hình của Phân tích tại thời điểm bạn xuất bản nó, bao gồm những thứ như bộ lọc, tham số, điều khiển và thứ tự sắp xếp.\nNội dung Phân tích dữ liệu với Athena Trực quan hóa với AWS QuickSight "
},
{
	"uri": "//localhost:1313/vi/7-clean-up/",
	"title": "Dọn dẹp tài nguyên",
	"tags": [],
	"description": "",
	"content": "Quicksight Xóa Visual QuickSight\nXóa Analyzes QuickSight\nAWS Glue Xóa Table database trong AWS Glue\nXóa Database trong AWS Glue\nXóa Notebook\nXóa development endpoints\nEventBridge Xóa EventBridge Lambda Function Xóa Lambda Function AWS S3 bucket Xóa S3 Bucket "
},
{
	"uri": "//localhost:1313/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]