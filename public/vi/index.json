[
{
	"uri": "//localhost:1313/vi/2-preparation-steps/1-glue-role/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "Chúng ta sẽ bắt đầu bằng việc tạo một Security Group cần thiết cho bài lab này.\nĐể xác định các rule Inbound cần có, chúng ta liệt kê ra các yêu cầu như sau:\nNgười dùng truy cập từ ngoài vào ứng dụng ShareNote thông qua cổng 80 bằng giao thức HTTP với Source IP bất kì. Load Balancer sẽ điều phối các yêu cầu này đến các server thông qua cổng 8080 với Source IP trong VPC. Các Application server sẽ giao tiếp với Database thông qua cổng 3306 với Source IP trong VPC. Chúng ta sẽ mở truy cập SSH để có thể kết nối đến instance để triển khai ứng dụng. Trên thực tế, chúng ta nên tạo các security group riêng biệt cho từng thành phần. Tuy nhiên trong khuôn khổ bài thực hành này, chúng ta sẽ sử dụng một security group (Các rule không trùng lẫn nhau).\nTạo Security Group Truy cập vào EC2 Management Console bằng cách gõ và chọn dịch vụ EC2 trong thanh tìm kiếm. Ở thanh điều hướng bên trái, click chọn Security Groups. Ở trang Security Groups, click chọn Create Security Group. Ở trang Create security group, thiết lập các thông số như sau: Mục Basic details: Security group name: Nhập vào tên security group (VD: sharenote-sg) Description: Nhập vào diễn giải của security group. (VD: Security for Sharenote app ) VPC: Chọn Default VPC. Bạn sẽ xây dựng bài lab này bên trong Default VPC. Trên thực tế, AWS khuyến cáo rằng bạn không không nên sử dụng Default VPC cho mục dích chạy môi trường Production. Tuy nhiên, bạn sẽ sử dụng Default VPC để được thuận tiện trong bài thực hành này.\nMục Inbound rules: Thêm các Inbound rule như đề cập ở trên. Chọn Add rule để thêm một rule. Type: HTTP | Source: Anywhere-IPv4 Type: Custom TCP | Port range: 8080 | Source: Anywhere-IPv4 Type: MySQL/Aurora | Source: Custom 172.31.0.0/16 (Default VPC CIDR block) Type: SSH | Source: My IP Chọn Create security group. Kiểm tra Security group đã được tạo thành công. Đến đây, chúng ta đã hoàn thành việc tạo Security Group, tiếp theo chúng ta sẽ tiến hành tạo ShareNote Database.\n"
},
{
	"uri": "//localhost:1313/vi/",
	"title": "Data Lake on AWS: Clean Data with Lambda Triggers",
	"tags": [],
	"description": "",
	"content": "Data Lake on AWS: Clean Data with Lambda Triggers Tổng quan Workshop này hướng dẫn bạn tạo một hệ thống Data Lake trên AWS để thu thập, xử lý, làm sạch và truy vấn dữ liệu. Chúng ta sẽ bắt đầu bằng việc thu thập dữ liệu từ API, upload vào S3, sau đó kết hợp AWS Lambda trigger để tự động làm sạch dữ liệu ngay khi tải lên. Dữ liệu sẽ được đổ vào Glue Data Catalog để phân tích bằng Athena, chuyển đổi qua Glue Job, và trực quan hóa với QuickSight.\nMục đích Tìm hiểu về Data Lake: Giúp người tham gia hiểu rõ Data Lake là gì, cách xây dựng và vận hành Data Lake trên AWS.\nTự động hóa quy trình làm sạch dữ liệu: Đồng bộ các dữ liệu upload lên S3 và xử lý ngay lập tức bằng Lambda trigger.\nTăng cường kỹ năng phân tích dữ liệu: Tìm hiểu Glue Data Catalog, truy vấn Athena, và biểu diễn trực quan QuickSight.\nĐộc lập và linh hoạt: Xây dựng mô hình hợp nhất cho nhiều loại dữ liệu, đầu vào từ API, S3, hay dữ liệu ngoại.\nYêu cầu: Tài khoản AWS: User có quyền Admin và đã thiết lập billing.\nDữ liệu API: Link dữ liệu sẽ được cung cấp trong workshop.\nChi phí: AWS Free Tier: S3, Lambda, Athena AWS Glue Job: 0.44$/h for one session QuickSight: 30 days free "
},
{
	"uri": "//localhost:1313/vi/1-introduction/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "Tổng quan Data Lake là một thuật ngữ chuyên môn có liên quan đến Big Data (Dữ liệu lớn). Data Lake đơn giản là nơi chứa dữ liệu thô (chưa xử lý) chờ được xử lý phân tích và đưa ra các đánh giá nhận xét (insight).\nData Lake có các tính chất sau:\nThu thập mọi thứ – chứa tất cả dữ liệu dạng thô hoặc đã được xử lý trong khoảng thời gian dài. Đa người dùng – cho phép nhiều người dùng tinh chỉnh, khám phá và làm phong phú dữ liệu. Truy cập linh hoạt – hỗ trợ nhiều cách thức truy cập dữ liệu (access pattern) trên cơ sở hạ tầng dùng chung: lô (batch), tương tác, trực tuyến, - tìm kiếm, trong bộ nhớ và các công cụ xử lý khác. EventBridge là một dịch vụ serverless sử dụng các sự kiện để kết nối các thành phần ứng dụng với nhau, giúp bạn dễ dàng xây dựng các ứng dụng hướng sự kiện có thể mở rộng quy mô. Amazon Glue: là một dịch vụ ETL hoàn chỉnh. Bạn có thể sử dụng Glue Crawler để nhận diện dữ liệu của bạn và lưu trữ thông tin dữ liệu (metadata) liên quan (ví dụ: định nghĩa bảng và schema) trong Glue Data Catalog. Sau khi được phân loại, dữ liệu của bạn có thể được tìm kiếm ngay, truy vấn và sẵn sàng cho các công việc ETL.\nAWS Glue ETL có thể tạo mã để thực hiện công việc chuyển đổi dữ liệu và đưa dữ liệu vào vùng lưu trữ. AWS Glue có khả năng tạo mã Python có thể tùy chỉnh, tái sử dụng.\nSau khi các ETL Job của bạn đã sẵn sàng, chúng ta có thể tạo lịch để chạy trên môi trường Apache Spark có khả năng mở rộng lớn được quản lý bởi AWS Glue. Amazon Athena: một dịch vụ truy vấn tương tác được sử dụng để phân tích dữ liệu trong Amazon S3 với SQL tiêu chuẩn. Chúng ta chỉ cần trỏ đến dữ liệu của bạn trong Amazon S3, xác định schema và bắt đầu truy vấn bằng trình chỉnh sửa truy vấn tích hợp. Amazon Athena cho phép chúng ta khai thác tất cả dữ liệu của mình trong Amazon S3 mà không cần phải thiết lập các quy trình ETL phức tạp. Amazon Athena tính tiền dựa trên các truy vấn được chạy.\nAmazon Athena sử dụng Presto với hỗ trợ SQL ANSI và hoạt động với nhiều định dạng dữ liệu tiêu chuẩn, bao gồm CSV, JSON, ORC, Avro và Parquet. Athena được khuyến nghị cho nhu cầu truy vấn nhanh, nhưng nó cũng có thể xử lý các phân tích phức tạp, bao gồm các phép Join với lượng dữ liệu lớn, các window functions và mảng. Amazon QuickSight: một dịch vụ biểu diễn dữ liệu được quản lý hoàn toàn bởi AWS. Data source là một kho lưu trữ dữ liệu bên ngoài và bạn cần cấu hình việc truy cập dữ liệu trong kho dữ liệu bên ngoài này, ví dụ. Amazon S3, Amazon Athena, Salesforce, v.v.\nDataset xác định dữ liệu cụ thể trong Data source mà bạn muốn sử dụng. Ví dụ: Data source có thể là một bảng nếu bạn đang kết nối với Data source cơ sở dữ liệu. Nó có thể là một file nếu bạn đang kết nối với Data source Amazon S3.\nAnalysis là nơi chứa một tập hợp các Visual và câu chuyện có liên quan, ví dụ như tất cả các câu chuyện áp dụng cho một mục tiêu kinh doanh nhất định hoặc KPI.\nVisual là một biểu diễn đồ họa cho dữ liệu của bạn. Bạn có thể tạo nhiều loại Visual khác nhau trong một analysis, sử dụng các bộ dữ liệu và loại Visual khác nhau.\nDashboard là một trang bao gồm một hoặc nhiều Analysis chỉ cho phép xem mà bạn có thể chia sẻ với những người dùng Amazon QuickSight khác cho mục đích báo cáo. Dashboard lưu giữ cấu hình của bản Analysis tại thời điểm bạn xuất bản nó, bao gồm những thứ như lọc, tham số, điều khiển và thứ tự sắp xếp.\n"
},
{
	"uri": "//localhost:1313/vi/6-analysis-visualization/1-analysis-athena/",
	"title": "Analysis with Athena",
	"tags": [],
	"description": "",
	"content": "Analysis with Athena As Athena uses the AWS Glue Catalog to track data sources, all tables in Glue can be queried through Athena.\nGo to the AWS Management Console\nFind Athena Select Athena In the Athena interface:\nData Source, select AwsDataCatalog Database, select summitdb Select processed-data table Select Preview Table "
},
{
	"uri": "//localhost:1313/vi/3-lambda-setup/1-creating-bucket/",
	"title": "Create an S3 Bucket",
	"tags": [],
	"description": "",
	"content": "Download data Click on this: Post data Save with Ctrl + S Click on this: Comment data Save with Ctrl + S On the file .json you just downloaded, you can change the code to testing Clean Data Use Case You can modify the data for testing purposes: delete some records, set certain fields to null, or change the id or postId values from numbers to strings.\nCreate S3 Bucket Visit the AWS Management Console\nSearch for S3 Select S3 In the S3 interface\nChoose buckets Select Create bucket In the Create bucket interface\nFor Bucket name, enter datalake-demo-bucket Click Create bucket Successfully created the bucket\nSelect the newly created bucket In the new bucket interface\nSelect Create folder In the Create folder interface\nFor Folder name, enter raw-data Successfully created the folder At folder raw-data\nCreate post and comment folders After successfully creating the 2 folders, we will not upload the files immediately but will proceed to the next step, which is Creating the Lambda Function to handle data cleaning\n"
},
{
	"uri": "//localhost:1313/vi/3-lambda-setup/2-creating-lambda/",
	"title": "Create Lambda function",
	"tags": [],
	"description": "",
	"content": "Creating lambda function Creating lambda function for post data Access the AWS Management Console\nSearch for Lambda Select Lambda Choose Create a function At function name: Clean-PostData\nAt runtime, choose Python 3.13 Change default execution role, choose Use an existing role\nChoose AWSLambdaRoleDefault\nClick Create function Add this code\nimport json\rimport boto3\rdef lambda_handler(event, context):\rtry:\rbucket_name = event[\u0026#39;detail][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;]\rfile_key = event[\u0026#39;detail\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;]\rresponse = s3.get_object(Bucket=bucket_name, Key=file_key)\rraw_data = response[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;)\rposts = json.loads(raw_data)\rcleaned_data = []\rfor post in posts:\rif (isinstance(post.get(\u0026#39;userId\u0026#39;), int) and\risinstance(post.get(\u0026#39;id\u0026#39;), int) and\risinstance(post.get(\u0026#39;title\u0026#39;), str) and\risinstance(post.get(\u0026#39;body\u0026#39;), str) and\rcleaned_data.append(post))\rcleaned_file_key = file_key.replace(\u0026#39;raw-data/post\u0026#39;, \u0026#39;cleaned-data\u0026#39;/post)\rcleaned_file_content = \u0026#34;\\n\u0026#34;.join(json.dumps(post) for post in cleaned_data)\rs3.put_object(Bucket=bucket_name, Key=cleaned_file_key, Body=cleaned_file_content)\rprint(f\u0026#34;Cleaned data uploaded to :s3://{bucket_name}/{cleaned_file_key}\u0026#34;)\rexcept Exception as e:\rprint(f\u0026#34;Error processing file {file_key}: {e}\u0026#34;)\rraise e Because AWS Glue cannot interpret JSON Array as JSON Objects for schema mapping, we need to convert the data into JSON Object format.\nThis code will check the incoming data, and if the data line is not in the correct format, it will skip that data.\nuserId must be an integer.\nid must be an integer.\ntitle must be a string.\nbody must be a string.\nClick Deploy(Ctrl+Shift+U) Creating lambda function for comment data Simirlar to Creating lambda function for post data, create a Lambda Function Clean-CommentData\nUsing this code:\nimport json\rimport boto3\rdef lambda_handler(event, context):\rtry:\rbucket_name = event[\u0026#39;detail][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;]\rfile_key = event[\u0026#39;detail\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;]\rresponse = s3.get_object(Bucket=bucket_name, Key=file_key)\rraw_data = response[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;)\rcomments = json.loads(raw_data)\rcleaned_data = []\rfor comment in comments:\rif (isinstance(post.get(\u0026#39;postId\u0026#39;), int) and\risinstance(post.get(\u0026#39;id\u0026#39;), int) and\risinstance(post.get(\u0026#39;name\u0026#39;), str) and\risinstance(post.get(\u0026#39;email\u0026#39;), str) and\risinstance(post.get(\u0026#39;body\u0026#39;), str) and\rcleaned_data.append(post))\rcleaned_file_key = file_key.replace(\u0026#39;raw-data/comment\u0026#39;, \u0026#39;cleaned-data\u0026#39;/comment)\rcleaned_file_content = \u0026#34;\\n\u0026#34;.join(json.dumps(comment) for post in cleaned_data)\rs3.put_object(Bucket=bucket_name, Key=cleaned_file_key, Body=cleaned_file_content)\rprint(f\u0026#34;Cleaned data uploaded to :s3://{bucket_name}/{cleaned_file_key}\u0026#34;)\rexcept Exception as e:\rprint(f\u0026#34;Error processing file {file_key}: {e}\u0026#34;)\rraise e "
},
{
	"uri": "//localhost:1313/vi/4-create-datalog/2-data-check/",
	"title": "Data check",
	"tags": [],
	"description": "",
	"content": "Data check Go to the AWS Management Console\nFind S3 Select S3 In the S3 interface\nSelect Buckets Select datalake-bucket-demo We will create a folder for Athena\nSelect Create folder In the Create folder interface\nFolder name, enter Athena Select Create folder Successfully created folder\nGo to the AWS Management Console\nFind Athena Select Athena In the Athena interface\nSelect View settings to set the path to store query results In the Amazon Athena interface\nSelect Settings Select Manage Select the path to the newly created Athena folder, then select Choose\nReturn to Manage settings interface, check again and select Save We use Amazon Athena to query data\nData Source, select AwsDataCatalog Database, select summitdb Select comment table Select Preview Table Make a query of 10 rows of data from the comment table in the database summitdb\nQuery successful Test data Checking data of comment table where postid = 10 Checking data of post table limit 10; Table comment join table post "
},
{
	"uri": "//localhost:1313/vi/2-preparation-steps/",
	"title": "Preparation Steps",
	"tags": [],
	"description": "",
	"content": "Preparation Steps We will prepare to create an IAM role for AWS Glue.\nContents Create Glue service role Create Lambda function role Create Policy "
},
{
	"uri": "//localhost:1313/vi/2-preparation-steps/2-lambda-role/",
	"title": "Tạo ShareNote Database",
	"tags": [],
	"description": "",
	"content": "Ở bước này, chúng ta sẽ khởi tạo một cơ sở dữ liệu (database) sử dụng AWS RDS cho ứng dụng ShareNote được triển khai ở bước tiếp theo.\nTạo cơ sở dữ liệu cho ShareNote với AWS RDS Truy cập vào RDS Management Console bằng cách gõ và chọn RDS trong thanh tìm kiếm. Chọn Create database. Lựa chọn các thông số thiết lập sau cho Database: (Các thông số không được nhắc đến sẽ giữ ở thiết lập mặc định). Mục Choose a database creation method: Chọn Standard create. Mục Engine options: Chọn MySQL. Ứng dụng ShareNote sử dụng MySQL nên chúng ta cũng sẽ lựa chọn MySQL Engine.\nKéo màn hình xuống dưới, mục Templates: Chọn Free tier. Mục Settings:\nDB instance identifier: nhập tên cho DB Instance (ví dụ: sharenote-db)\nSettings:\nMaster username: thiết lập tài khoản master dùng để truy cập vào cơ sở dữ liệu (ví dụ: admin). Master password: thiết lập mật khẩu với ít nhất 8 kí tự và hạn chế nhập kí tự đặc biệt (ví dụ: admin123). Confirm password: Nhập lại mật khẩu ở phía trên. Nếu bạn nhập kí tự đặc biệt (ví dụ: ! hoặc @ hoặc #), khi bạn kết nối tới cơ sở dữ liệu trên DB Instance từ EC2 Instance sử dụng hệ điều hành Ubuntu ở phần sau, bạn sẽ phải thực hiện thao tác bỏ qua (escape) đối với các kí tự đặc biệt ấy, và điều đó sẽ gia tăng độ phức tạp của bài lab này một cách không cần thiết.\nTiếp tục kéo màn hình xuống, mục DB instance Class: dùng để chọn phân loại DB Instance. Click chọn phân loại Burstatble. Click chọn DB instance class : db.t2.micro. Giữ nguyên tùy chọn cho mục Storage. Tiếp tục kéo màn hình xuống, mục Connectivity: Virtual private cloud (VPC): giữ nguyên VPC mặc định. Subnet group: giữ nguyên Subnet group mặc định. Public access: chọn No VPC security group: Chọn Choose existing. Existing VPC security groups: Chọn tên Security Group mà bạn đã tạo ở phần 1 (ví dụ: sharenote-sg và để nguyên default security group.) Availability Zone: chọn ap-southeast-1a Kéo màn hình xuống và click để mở rộng mục Additional configuration. Tại mục Database options phần Initial database name: Nhập vào tên của database sẽ được khởi tạo ban đầu (ví dụ: NoteDB) Các phần còn lại, bạn hãy giữ nguyên mặc định. Kéo màn hình xuống cuối trang và click Create database.\nDatabase sẽ được khởi tạo và ở trạng thái Creating. Hãy đợi cho Status chuyển sang Available để có thể sử dụng.\nClick sharenote-db trong danh sách Databases.\nỞ trang thông tin database, tab Connectivity \u0026amp; security, ghi chú lại Endpoint của database. Chúng ta sẽ sử dụng thông tin này cho việc cấu hình ở bước kế tiếp.\nTrong bước này chúng ta đã hoàn thành việc tạo Database cho ShareNote, tiếp theo chúng ta sẽ tạo một EC2 instance để tiến hành cài đặt ứng dụng ShareNote.\n"
},
{
	"uri": "//localhost:1313/vi/6-analysis-visualization/2-visualize-quicksight/",
	"title": "Visualize with QuickSight",
	"tags": [],
	"description": "",
	"content": "Visualize with QuickSight In this step, we will perform visualization using QuickSight.\nLog in to the Amazon QuickSight Console and complete the registration process. You can refer to the registration process here.\nAccess the AWS Management Console\nFind QuickSight Select QuickSight Note: Pay attention to the Region you are using. Check and switch the Region if necessary to avoid errors.\nIn the QuickSight interface, select Manage QuickSight Configure Permissions\nSelect Security \u0026amp; permissions Select Manage Select the services and choose Amazon S3\nSelect S3 Buckets Linked To QuickSight Account Choose datalake-bucket-demo Select Finish Select and review the Role and services. Then, select Save In the QuickSight interface:\nSelect Datasets Select New dataset In the Create dataset interface:\nChoose the source from Athena Configure the data source:\nData source name, enter summitdemo Choose Validate Connection. If the connection is successful, it will display SSL is enabled Select Create data source Choose a Table:\nCatalog, select AWSDataCatalog Database, enter summitdb Table, select processed_data Select Select In the Finish dataset creation step:\nSelect Directly query your data Select Visualize Select Create Use Amazon QuickSight to visualize the transformed data:\nCount comments each post\nClick on Pie Chart Group/color: postid Value: commentid(Count) Count post each user\nGroup/color: postuserid Value: postid(Count) If you meet error like or Go to IAM Role Console Find aws-quicksight-service-role-\u0026hellip; Click Add Permissions Click Attach policies Search for Quicksight and choose these policies: "
},
{
	"uri": "//localhost:1313/vi/4-create-datalog/1-create-glue-crawler/",
	"title": "Create AWS Glue Crawler",
	"tags": [],
	"description": "",
	"content": "Create Glue Crawler Access the AWS Management Console Find AWS Glue. Select AWS Glue. In the AWS Glue interface, select Crawlers. Choose Create Crawler. In the Add Crawler interface, enter Crawler name as summitcrawler and select Next. For Add data source, select S3. Choose S3 path through Browse. Choose: datalate-bucket-demo/cleaned-data. Also, select Crawl new sub-folders only and Add an S3 data source. After adding the data source, select Next. For IAM role, choose AWSGlueServiceDefault. Then, select Next. For Target database, perform Add database. Create a database by entering the database name as summitdb and selecting Create database. After creating the database, select the database and choose Next. Review the configuration and select Create crawler. Crawler creation successful. Then, choose Run crawler. It takes about 2-3 minutes to run the crawler. When you see the crawler status as Ready. In the AWS Glue interface, select Table, and you will see 2 data tables. Select the post data table. Select the comment data table. Explore the details of the data table. "
},
{
	"uri": "//localhost:1313/vi/3-lambda-setup/",
	"title": "Setup Data Processing Pipeline",
	"tags": [],
	"description": "",
	"content": "Setup Data Processing Pipeline Steps: 3.1. Create an S3 Bucket 3.2. Create Lambda Function 3.3. Setup EventBridge and S3 Trigger 3.4. Upload data and view the results "
},
{
	"uri": "//localhost:1313/vi/3-lambda-setup/3-creating-eventbridge/",
	"title": "Setup EventBridge and S3 Trigger",
	"tags": [],
	"description": "",
	"content": "Creating EventBridge Access the AWS Management Console\nSearch for EventBridge Select Amazon EventBridge At the Amazon EventBridge Console, choose Create Rule At name: TriggerCommentData Click Next At creation method, choose Custom pattern At event pattern: {\r\u0026#34;source\u0026#34;: [\u0026#34;aws.s3\u0026#34;],\r\u0026#34;detail-type\u0026#34;: [\u0026#34;Object Created\u0026#34;],\r\u0026#34;detail\u0026#34;:{\r\u0026#34;bucket\u0026#34;:{\r\u0026#34;name\u0026#34;: [\u0026#34;datalake-bucket-demo\u0026#34;]\r},\r\u0026#34;object\u0026#34;:{\r\u0026#34;key\u0026#34;:[{\r\u0026#34;prefix\u0026#34;: \u0026#34;raw-data/comment\u0026#34;\r}]\r}\r}\r} Click Next At Select a target: Lambda function Function: Clean-CommentData Click Next Review rule, click Create rule Similar to Step 2-6, Create TriggerPostData rule\nUsing this pattern:\n{\r\u0026#34;source\u0026#34;: [\u0026#34;aws.s3\u0026#34;],\r\u0026#34;detail-type\u0026#34;: [\u0026#34;Object Created\u0026#34;],\r\u0026#34;detail\u0026#34;:{\r\u0026#34;bucket\u0026#34;:{\r\u0026#34;name\u0026#34;: [\u0026#34;datalake-bucket-demo\u0026#34;]\r},\r\u0026#34;object\u0026#34;:{\r\u0026#34;key\u0026#34;:[{\r\u0026#34;prefix\u0026#34;: \u0026#34;raw-data/post\u0026#34;\r}]\r}\r}\r} After create TriggerPostData rule, we will have: Comeback to the Clean-PostData and Clean-CommentData Lambda Function, you will see: Setup send notifications to EventBridge for all events in S3 Open the S3 datalake-bucket-demo console, Click Properties At Amazon EventBridge, click Edit Turn on Send notifications to EventBridge for all events in this bucket, then click Save changes Congratulations, you\u0026rsquo;ve successfully created a trigger to activate Lambda when listening to the S3 upload event with EventBridge. Now we\u0026rsquo;ll move on to the next step: Uploading data and viewing the results. "
},
{
	"uri": "//localhost:1313/vi/2-preparation-steps/3-policy/",
	"title": "Tạo Policy",
	"tags": [],
	"description": "",
	"content": "Tạo Policy Truy cập AWS Management Console\nTrước tiên, bạn cần truy cập AWS Management Console tại https://aws.amazon.com/console/. Đăng nhập vào tài khoản AWS\nĐăng nhập vào tài khoản AWS của bạn bằng tên đăng nhập và mật khẩu. Mở trang IAM (Identity and Access Management)\nSau khi đăng nhập thành công, chọn dịch vụ “IAM” bằng cách tìm kiếm nó trong thanh tìm kiếm hoặc tìm trong danh sách các dịch vụ. Tạo một Policy\nTrong giao diện IAM, chọn “Policies” ở menu bên trái. Bấm vào nút “Create policy” để tạo một policy mới. Paste JSON Policy\nTrong trang “Create policy”, chọn phần “JSON” và dán nội dung JSON policy bạn đã cung cấp ở trên vào ô văn bản. Đảm bảo rằng JSON policy đã được đánh dấu là hợp lệ và không có lỗi cú pháp. {\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: \u0026#34;iam:PassRole\u0026#34;,\r\u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:iam::AccountID:role/AWSGlueServiceRoleDefault\u0026#34;\r}\r]\r} Đặt tên cho Policy Tiếp theo, bạn cần đặt tên cho policy. Điền tên vào trường “Name”. Cung cấp mô tả (tùy chọn) cho policy nếu bạn muốn. Kiểm tra và tạo Policy Bấm vào nút “Review policy” để kiểm tra lại thông tin policy của bạn. Sau khi kiểm tra kỹ, nếu không có lỗi, hãy bấm vào nút “Create policy” để tạo policy mới. Đính kèm Policy vào AWSGlueServiceRoleDefault Sau khi policy được tạo thành công, bạn cần đính kèm nó vào role AWSGlueServiceRoleDefault. Chọn “Roles” ở menu bên trái trong giao diện IAM. Tìm và chọn role “AWSGlueServiceRoleDefault”. Trong trang chi tiết của role, chọn tab “Permissions” và sau đó bấm vào nút “Add inline policy”. Trong trang “Create policy”, tìm kiếm và chọn policy bạn vừa tạo. Bấm vào nút “Next: Review policy” để kiểm tra lại. Sau khi kiểm tra và không có lỗi, hãy bấm vào nút “Create policy” để đính kèm policy vào role. "
},
{
	"uri": "//localhost:1313/vi/4-create-datalog/",
	"title": "Create Data Catalog",
	"tags": [],
	"description": "",
	"content": "Create Data Catalog Contents: 4.1. Create Glue Crawler\n4.2. Data Check\n"
},
{
	"uri": "//localhost:1313/vi/5-data-transformation/",
	"title": "Create Data Catalog",
	"tags": [],
	"description": "",
	"content": "Create Data Catalog Contents: 4.1. Create Glue Crawler\n4.2. Data Check\n"
},
{
	"uri": "//localhost:1313/vi/6-analysis-visualization/",
	"title": "Create Data Catalog",
	"tags": [],
	"description": "",
	"content": "Create Data Catalog Contents: 4.1. Create Glue Crawler\n4.2. Data Check\n"
},
{
	"uri": "//localhost:1313/vi/3-lambda-setup/4-watch-result/",
	"title": "Upload data and view the results",
	"tags": [],
	"description": "",
	"content": "Upload data Open folder: datalake-bucket-demo/raw-data/post Click Upload Click Add files Choose file post.json you have downloaded at 3.1. Create S3 Bucket Click Upload Open the Lambda Function Clean-PostData, at Monitor, click View CloudWatch logs Select the latest Log stream based on timestamp. Inside the log events of the selected Log Stream, look for the message:\nCleaned data uploaded to s3://datalake-bucket-demo/cleaned-data/post/post.json. That means your Lambda function has successfully executed and uploaded the cleaned data. Go back to the S3 Console and navigate to the datalake-bucket-demo bucket. Locate the folder cleaned-data/post/. Inside this folder, you will find a file named post.json. Download the post.json file to review its contents.\nYou will notice that the data has been cleaned, with any invalid or malformed entries removed, and structured correctly as JSON objects.\nThis ensures the data is ready for AWS Glue to infer the schema and process it accurately.\nMy raw-data/post.json: In my raw-data post.json, you will find:\r1. The first object is missing the body field.\r2. The second object is missing the title field.\r3. The id field in the third object is a string instead of a number.\r4. The userId field in the fourth object is a string instead of a number.\r5. The raw-data file is an array of JSON objects structured as:\r[{Object 1}, {Object 2}, ..., {Object n}].\rMy cleaned-data/post.json In my cleaned-data post.json, you will see:\r1. The 1st, 2nd, 3rd, and 4th objects have been removed.\r2. The data is now formatted as {Object 1}, {Object 2}, ..., {Object n} without being wrapped in an array.\rSimilar to upload post.json, upload comment.json in raw-data/comment/.\nThat’s how you clean the data using AWS Lambda and EventBridge. Now, let’s move to the next step: Create the Data Catalog.\n"
},
{
	"uri": "//localhost:1313/vi/7-clean-up/",
	"title": "Dọn dẹp tài nguyên",
	"tags": [],
	"description": "",
	"content": "Chúng ta sẽ dọn dẹp tài nguyên theo thứ tự như sau:\nXóa Auto Scaling Group: Truy cập EC2 Management Console Trên thanh điều hướng bên trái, chọn Auto Scaling Groups Chọn Auto Scaling Group liên quan tới bài lab. Click Delete Gõ delete vào ô trống và nhấn delete Xóa Load Balancer: Truy cập EC2 Management Console Trên thanh điều hướng bên trái, chọn Load Balancers Chọn Load Balancer liên quan tới bài lab. Click Actions. Click Delete. Xóa Target Group: Truy cập EC2 Management Console Trên thanh điều hướng bên trái, chọn Target Groups Chọn Target Group liên quan tới bài lab. Click Actions. Click Delete. Click Yes, delete Xóa Launch Template: Truy cập EC2 Management Console Trên thanh điều hướng bên trái, chọn Launch Templates Chọn Launch Template liên quan tới bài lab. Click Actions. Click Delete template Gõ delete vào ô trống và nhấn delete Xóa AMI: Truy cập EC2 Management Console Trên thanh điều hướng bên trái, chọn AMIs Chọn AMI liên quan tới bài lab. Click Actions. Click Deregister. Click Continue. Terminate EC2 instance Truy cập EC2 Management Console Trên thanh điều hướng bên trái, chọn Intances Chọn tất cả EC2 Instance liên quan tới bài lab. Click Actions. Click Manage Instance State. Chọn Terminate. Click Change State Xóa DB Instance Truy cập RDS Management Console Trên thanh điều hướng bên trái, chọn Databases Chọn tất cả DB Instance liên quan tới bài lab. Click Actions. Click Delete Bỏ chọn Create final snapshot? và chọn I acknowledge that upon instance deletion, automated backups, including system snapshots and point-in-time recovery, will no longer be available Gõ delete me vào ô trống. Click Delete Xóa Security Group Truy cập EC2 Management Console Trên thanh điều hướng bên trái, chọn Security Groups Chọn tất cả Security Groups liên quan tới bài lab. Click Actions. Click Delete security groups Click Delete "
},
{
	"uri": "//localhost:1313/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]